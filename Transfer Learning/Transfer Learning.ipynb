{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b480567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63442fa5",
   "metadata": {},
   "source": [
    "#### Make prediction from already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1a4d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ameen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ameen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (224, 224)\n",
    "\n",
    "classifier = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\", input_shape=IMAGE_SHAPE+(3,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae03c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
      "228813984/228813984 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url,  cache_dir='.', untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f4cd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\datasets\\\\flower_photos'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b8c1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('datasets/flower_photos')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79c1294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('datasets/flower_photos/daisy/100080576_f52e8ee070_n.jpg'),\n",
       " WindowsPath('datasets/flower_photos/daisy/10140303196_b88d3d6cec.jpg'),\n",
       " WindowsPath('datasets/flower_photos/daisy/10172379554_b296050f82_n.jpg'),\n",
       " WindowsPath('datasets/flower_photos/daisy/10172567486_2748826a8b.jpg'),\n",
       " WindowsPath('datasets/flower_photos/daisy/10172636503_21bededa75_n.jpg')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_dir.glob('*/*.jpg'))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02478d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1723e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('datasets/flower_photos/roses/10090824183_d02c613f10_m.jpg'),\n",
       " WindowsPath('datasets/flower_photos/roses/102501987_3cdb8e5394_n.jpg'),\n",
       " WindowsPath('datasets/flower_photos/roses/10503217854_e66a804309.jpg'),\n",
       " WindowsPath('datasets/flower_photos/roses/10894627425_ec76bbc757_n.jpg'),\n",
       " WindowsPath('datasets/flower_photos/roses/110472418_87b6a3aa98_m.jpg')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roses = list(data_dir.glob('roses/*'))\n",
    "roses[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f23fadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_images_dict = {\n",
    "    'roses': list(data_dir.glob('roses/*')),\n",
    "    'daisy': list(data_dir.glob('daisy/*')),\n",
    "    'dandelion': list(data_dir.glob('dandelion/*')),\n",
    "    'sunflowers': list(data_dir.glob('sunflowers/*')),\n",
    "    'tulips': list(data_dir.glob('tulips/*')),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b3b604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('datasets/flower_photos/roses/10090824183_d02c613f10_m.jpg'),\n",
       " WindowsPath('datasets/flower_photos/roses/102501987_3cdb8e5394_n.jpg'),\n",
       " WindowsPath('datasets/flower_photos/roses/10503217854_e66a804309.jpg'),\n",
       " WindowsPath('datasets/flower_photos/roses/10894627425_ec76bbc757_n.jpg'),\n",
       " WindowsPath('datasets/flower_photos/roses/110472418_87b6a3aa98_m.jpg')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowers_labels_dict = {\n",
    "    'roses': 0,\n",
    "    'daisy': 1,\n",
    "    'dandelion': 2,\n",
    "    'sunflowers': 3,\n",
    "    'tulips': 4,\n",
    "}\n",
    "flowers_images_dict['roses'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dcc7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(str(flowers_images_dict['roses'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c6f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for flower_name, images in flowers_images_dict.items():\n",
    "    for image in images:\n",
    "        img = cv2.imread(str(image))\n",
    "        resized_img = cv2.resize(img,(224,224))\n",
    "        X.append(resized_img)\n",
    "        y.append(flowers_labels_dict[flower_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e73596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab33733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train_scaled = X_train / 255\n",
    "X_test_scaled = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7d0f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "\n",
    "pretrained_model_without_top_layer = hub.KerasLayer(\n",
    "    feature_extractor_model, input_shape=(224, 224, 3), trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff2865ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  (None, 1280)              2257984   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 6405      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_of_flowers = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  pretrained_model_without_top_layer,\n",
    "  tf.keras.layers.Dense(num_of_flowers)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e75a304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "86/86 [==============================] - 134s 1s/step - loss: 0.7609 - acc: 0.7111\n",
      "Epoch 2/5\n",
      "86/86 [==============================] - 89s 1s/step - loss: 0.3937 - acc: 0.8670\n",
      "Epoch 3/5\n",
      "86/86 [==============================] - 89s 1s/step - loss: 0.3072 - acc: 0.9001\n",
      "Epoch 4/5\n",
      "86/86 [==============================] - 88s 1s/step - loss: 0.2598 - acc: 0.9193\n",
      "Epoch 5/5\n",
      "86/86 [==============================] - 86s 1s/step - loss: 0.2223 - acc: 0.9335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2336cf5c520>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "  optimizer=\"adam\",\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['acc'])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdcaa626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 71s 1s/step - loss: 0.3880 - acc: 0.8617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3879563808441162, 0.8616557717323303]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d77f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
